                  A&P Software Inventory System   (introduction)
                  -----------------------------


      This system was designed by my former boss to suit some very
special conditions that he wanted to implement in his data center
design.  Therefore, the system will possibly be useful (as it stands)
for certain types of MVS sites.

      Nonetheless, the system has, in my opinion, a large educational
value.  However, if a site wants to implement this system and maintain
it, they really should have some definite goals in the way they do
things.  Those goals shouldn't be radically different from my former
boss's goals.

      Also, to implement this system, you need to have at least a
fairly competent SAS programmer on site.  SAS has some concepts and
methods that are very foreign to programmers trained only in
Assembler, and perhaps some other higher level languages such as
COBOL.  I don't think that a site should try to work with the "report"
side of this package, without a person around who is competent in SAS.

      This package is currently running with SAS Version 6.09.  If
you want to run it on a later version of SAS, you have to know what
you're doing.


DATA CENTER SETUP GOALS

      The following are some of the conditions in the data center,
where this system was set up:


1.  SMP/E is done to libraries that have a site-determined high-level
    qualifier.  At this site, this hlq is STDL.  BTW, the SMP/E
    control libraries have a hlq of SYSSMPE.

2.  IBM-named linklist and lpalist libraries are not used, and are
    empty, for the most part.  Almost all system modules are copied
    into big system libraries:  SYS1.LINKMVS and SYS1.LPAMVS.  Program
    product modules (non-IBM) are all copied into large load libraries:
    SPP.LINKLIB and SPP.LPALIB.  The SPP libraries are concatenated
    just behind the system libraries.

3.  LPAR-specific linklist and lpalist modules are in lpar-specific
    libraries concatenated just ahead of the big system libraries,
    and they don't contain too much--only modules necessary for
    lpar specificity.

4.  ISPF libraries are combined too, with IBM libraries just ahead
    of the large SPP program product libraries.

5.  As many ISPF panels as possible, are ISPPREP'ed before being placed
    into the large panel libraries.  This is for speed in execution.


      This arrangement was designed to speed execution, and promote
short response times, as much as possible.  In my experience, the
machines do run fast, and response times are very good.  As an aside,
IPLs are rare, and are done months apart, usually.


 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

      Since IBM packaging nowadays is relying on LLA and VLF to
create execution efficiency, IBM usually recommends concatenating
very many libraries, in a manner opposite to this design.

      Because most modules are not executed from their IBM-supplied
libraries, in this design, Paul has created the concept of "Execution
Libraries".  This is in addition to the concepts of Target Libraries
and Distribution Libraries.  In this data center setup, Target
Libraries are almost always NOT executed from.

      Paul's Software Inventory Package (featured in this file)
therefore has to keep track of which libraries are executing, and how
their contents relates to the contents of the Target Libraries
(SMP/E-fed), from whence they came.

      With that in mind, I'll try to briefly explain the pieces of
this Software Inventory System, how they are set up, and how they
work.


 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -


      There are two pieces to this Software Inventory System:

1.  The Data Gatherer part.       Jobs:  SBGSTG1, SBGSTG2
2.  The SAS Reporting part.       Jobs:  SBGPRE,  SBGPRCR

      The Data Gatherer part can work anywhere.  It runs against
your disk volumes, to report on the software-containing datasets
which each volume contains.

      Stage 1 of the Data Gatherer creates a file of dataset names
included in a FORMAT1 VTOC dump for each dataset.  A file for each disk
pack is created by the Stage 1 process, and it is stored in a special
format pds, with a member name for each pack read.  Each member of the
Stage 1 pds contains a header record for the volume, followed by a
record containing an image of each of the actual FORMAT1 VTOC entries
for all the datasets.

Program for Stage 1:   WVOLCOLL

      Stage 2 of the Data Gatherer creates a file of dataset contents,
divided by source datasets and load datasets.  VSAM datasets, and such,
are bypassed by the Stage 2 processing.  Only datasets that are assumed
to contain software, are included.  Stage 2 processing is fed from the
contents of the Stage 1 pds.  Stage 2 produces a special-format pds
also, but with member-level data for each scanned volume.

      Every pds on the scanned volume, which is assumed to be
software-containing, is analyzed member by member, to produce the
Stage 2 output for that disk volume.  Separate processing is done in
Stage 2, for source datasets, and separate processing for load library
datasets.  Inside the Stage 2 output file for a volume, there are
several different types of records, depending on whether a program is
source code, or whether it is a load module.

Programs for Stage 2:  WVOLCOL0   (analyze load module pds'es)
                       WVOLCOL1   (analyze source pds'es)


      We'll deal with the SAS Reporting part later.  The Data
Gathering part is really independent of the SAS Reporting part, but
NOT vice-versa.

      In the SAS reporting part, we will set up formats and use
control cards, to tell the system which libraries are Execution
Libraries, which are Target Libraries, which are Distribution Libraries,
and which are SMP/E control libraries.  These control cards and formats
(the main format contains explanations for all the FMIDs) must be set up
by the programmer, according to what is at that site.  (It's kind of
painstaking, but you can follow our current models for hints.)

      The end result of the SAS part, will be to produce several
reports:

Report 1:  Shows duplicate LPA and LINK list modules, several ways:
           a- A module is in 2 Link List libraries
           b- A module is in a Link List and an LPA List library as well
           c- Same named modules which are different, exist in
              different libraries.

Report 2:  Execution Library to Target Library Match report.
           This report goes module-name by module-name.
           (There may be more than one execution library containing
           a given module.)

Report 3:  Which Execution Library modules are not matched with
           Target Library modules (you have to ask why they are
           different, or why they don't exist in the Target
           libraries).


      It is obvious that these are useful reports, even for an
installation whose setup is closer to the normal IBM standard.



